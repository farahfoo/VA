---
title: "In class exericse 09 "
description: |
  Text Visualisation. Text sensing and Text Mining are different
author:
  - name: Frostbear 
    url: https://sg.linkedin.com/in/farahfoo
    affiliation: SMU Masters in IT business (Fintech and Analytics)
    affiliation_url: https://scis.smu.edu.sg/master-it-business
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 4
    toc_float: true
    code_folding: FALSE
---

![Dealing with tidying text]("C:/farahfoo/VA/Pictures/tidyingtext.PNG")


# WORD CLOUD

various methods of sensing are tag cloud, wordle, word tree and phrase nets, story tracker. The rpackage used is ggwordcloud from the ggplot2 package and textplot, textnets (for correlation of the data), LDAvis (to create themes out of text data)

[word cloud](https://cran.r-project.org/web/packages/wordcloud/)
[htmlwidget](https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.html)
[ggwordcloud](https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html)


## Loading required packages

```{r echo = TRUE, message = FALSE}

packages = c('tidytext', 
             'widyr', 'wordcloud',
             'DT', 'ggwordcloud', 
             'textplot', 'lubridate', 
             'hms','tidyverse', 
             'tidygraph', 'ggraph',
             'igraph')
for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```

## Importing data files and preparing

REading all files from a folder into a data frame. read_lines is used to read up to max lines from a file. map command transforms input to return as an object. unnest flattens the list of columns into regular columns. mutate function adds new variables. transmute add and drop variables. read_rds extracts the combined data frame into an rds file

```{r echo = TRUE, message = FALSE}

news20 <- "data/20news/"

read_folder <- function(infolder) {
  tibble(file = dir(infolder, 
                    full.names = TRUE)) %>%
    mutate(text = map(file, 
                      read_lines)) %>%
    transmute(id = basename(file), 
              text) %>%
    unnest(text)
}

```

## Reading in all the messages from the folder

This step is transforming each line of the word file into an excel row

```{r echo = TRUE, message = FALSE}

raw_text <- tibble(folder = 
                     dir(news20, 
                         full.names = TRUE)) %>%
  mutate(folder_out = map(folder, 
                          read_folder)) %>%
  unnest(cols = c(folder_out)) %>%
  transmute(newsgroup = basename(folder), 
            id, text)
write_rds(raw_text, "data/rds/news20.rds")

write_csv(raw_text, "data/rds/news20.csv")

```

![How R imports the text file, 1 line into 1 row]("C:/farahfoo/VA/Pictures/importing text.PNG")

## initial EDA

```{r echo = TRUE, message = FALSE}

#raw_text <- "data/rds/news20.rds"

raw_text %>%
  group_by(newsgroup) %>%
  summarize(messages = n_distinct(id)) %>%
  ggplot(aes(messages, newsgroup)) +
  geom_col(fill = "lightblue") +
  labs(y = NULL)
       
head (raw_text,3)
glimpse (raw_text)

```

## Step 1: Removing header and automated email signitures

```{r echo = TRUE, message = FALSE}

cleaned_text0 <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") > 0,
         cumsum(str_detect(
           text, "^--")) == 0) %>%
  ungroup()

head (cleaned_text0,3)
glimpse (cleaned_text0)

```

## Step 2: Removing lines with nested text representing quotes from other users.

regular expressions are used to remove with nested text representing quotes eg. email addresses, article references

stringr detects presence / absence of a string. filter command used to retain rows desired.

```{r echo = TRUE, message = FALSE}

cleaned_text <- cleaned_text0 %>%
  filter(str_detect(text, "^[^>]+[A-Za-z\\d]")
         | text == "",
         !str_detect(text, 
                     "writes(:|\\.\\.\\.)$"),
         !str_detect(text, 
                     "^In article <")
  )

head (cleaned_text,3)
glimpse (cleaned_text)

```

dataset is split into tokens to remove stop words eg. conjunctions, the, a, he, she etc...

```{r echo = TRUE, message = FALSE}

usenet_words <- cleaned_text %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)

head (usenet_words,3)
glimpse (usenet_words)
```

After all the above processing, the headers, signatures, and formatting should have been removed, then we can start to analyse the common words in the dataset


## counting by all words and adding count as a column

```{r echo = TRUE, message = FALSE}

usenet_words %>%
  count(word, sort = TRUE)

words_by_newsgroup <- usenet_words %>%
  count(newsgroup, word, sort = TRUE) %>%
  ungroup()

head(words_by_newsgroup,3)
```


## Fun Visualisation

```{r echo = TRUE, message = FALSE}

wordcloud(words_by_newsgroup$word,
          words_by_newsgroup$n,
          max.words = 100)
```

breaking up the cloud by the source newsgroup

```{r echo = TRUE, message = FALSE}

set.seed(1234)
words_by_newsgroup %>%
  filter(n > 0) %>%
ggplot(aes(label = word,
           size = n)) +
  geom_text_wordcloud() +
  theme_minimal() +
  facet_wrap(~newsgroup)
```


