---
title: "In class exericse 09 "
description: |
  Text Visualisation. Text sensing and Text Mining are different
author:
  - name: Frostbear 
    url: https://sg.linkedin.com/in/farahfoo
    affiliation: SMU Masters in IT business (Fintech and Analytics)
    affiliation_url: https://scis.smu.edu.sg/master-it-business
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 4
    toc_float: true
    code_folding: FALSE
---

![Dealing with tidying text]("C:/farahfoo/VA/Pictures/tidyingtext.PNG")


# WORD CLOUD

various methods of sensing are tag cloud, wordle, word tree and phrase nets, story tracker. The rpackage used is ggwordcloud from the ggplot2 package and textplot, textnets (for correlation of the data), LDAvis (to create themes out of text data)

[word cloud](https://cran.r-project.org/web/packages/wordcloud/)
[htmlwidget](https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.html)
[ggwordcloud](https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html)


## Loading required packages

```{r echo = TRUE, message = FALSE}

packages = c('tidytext', 
             'widyr', 'wordcloud',
             'DT', 'ggwordcloud', 
             'textplot', 'lubridate', 
             'hms','tidyverse', 
             'tidygraph', 'ggraph',
             'igraph')
for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```

## Importing data files and preparing

REading all files from a folder into a data frame. read_lines is used to read up to max lines from a file. map command transforms input to return as an object. unnest flattens the list of columns into regular columns. mutate function adds new variables. transmute add and drop variables. read_rds extracts the combined data frame into an rds file

```{r echo = TRUE, message = FALSE}

news20 <- "data/20news/"

read_folder <- function(infolder) {
  tibble(file = dir(infolder, 
                    full.names = TRUE)) %>%
    mutate(text = map(file, 
                      read_lines)) %>%
    transmute(id = basename(file), 
              text) %>%
    unnest(text)
}

```

## Reading in all the messages from the folder

This step is transforming each line of the word file into an excel row

```{r echo = TRUE, message = FALSE}

raw_text <- tibble(folder = 
                     dir(news20, 
                         full.names = TRUE)) %>%
  mutate(folder_out = map(folder, 
                          read_folder)) %>%
  unnest(cols = c(folder_out)) %>%
  transmute(newsgroup = basename(folder), 
            id, text)
write_rds(raw_text, "data/rds/news20.rds")

write_csv(raw_text, "data/rds/news20.csv")

```

![How R imports the text file, 1 line into 1 row]("C:/farahfoo/VA/Pictures/importing text.PNG")

## initial EDA

```{r echo = TRUE, message = FALSE}

#raw_text <- "data/rds/news20.rds"

raw_text %>%
  group_by(newsgroup) %>%
  summarize(messages = n_distinct(id)) %>%
  ggplot(aes(messages, newsgroup)) +
  geom_col(fill = "lightblue") +
  labs(y = NULL)
       
head (raw_text,3)
glimpse (raw_text)

```

## Step 1: Removing header and automated email signitures

```{r echo = TRUE, message = FALSE}

cleaned_text0 <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") > 0,
         cumsum(str_detect(
           text, "^--")) == 0) %>%
  ungroup()

head (cleaned_text0,3)
glimpse (cleaned_text0)

```

## Step 2: Removing lines with nested text representing quotes from other users.

regular expressions are used to remove with nested text representing quotes eg. email addresses, article references

stringr detects presence / absence of a string. filter command used to retain rows desired.

```{r echo = TRUE, message = FALSE}

cleaned_text <- cleaned_text0 %>%
  filter(str_detect(text, "^[^>]+[A-Za-z\\d]")
         | text == "",
         !str_detect(text, 
                     "writes(:|\\.\\.\\.)$"),
         !str_detect(text, 
                     "^In article <")
  )

head (cleaned_text,3)
glimpse (cleaned_text)

```

dataset is split into tokens to remove stop words eg. conjunctions, the, a, he, she etc...

```{r echo = TRUE, message = FALSE}

usenet_words <- cleaned_text %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)

head (usenet_words,3)
glimpse (usenet_words)
```

After all the above processing, the headers, signatures, and formatting should have been removed, then we can start to analyse the common words in the dataset


## counting by all words and adding count as a column

```{r echo = TRUE, message = FALSE}

usenet_words %>%
  count(word, sort = TRUE)

words_by_newsgroup <- usenet_words %>%
  count(newsgroup, word, sort = TRUE) %>%
  ungroup()

head(words_by_newsgroup,3)
```


## Fun Visualisation

```{r echo = TRUE, message = FALSE}

wordcloud(words_by_newsgroup$word,
          words_by_newsgroup$n,
          max.words = 100)
```

breaking up the cloud by the source newsgroup. 

```{r echo = TRUE, message = FALSE}

set.seed(1234)
words_by_newsgroup %>%
  filter(n > 10) %>%
ggplot(aes(label = word,
           size = n)) +
  geom_text_wordcloud() +
  theme_minimal() +
  facet_wrap(~newsgroup)
```

# [Term frequency](https://en.wikipedia.org/wiki/Text_corpus) is a numerical statistic that is intended to reflect how important a word is to a document

## Binning data

The code chunk below uses bind_tf_idf() of tidytext to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.

```{r echo = TRUE, message = FALSE}

tf_idf <- words_by_newsgroup %>%
  bind_tf_idf(word, newsgroup, n) %>%
  arrange(desc(tf_idf))

head(tf_idf,3)
glimpse(tf_idf)
```

## Visualising tf-idf as interactive table

customising the data table: [https://rstudio.github.io/DT/functions.html](https://rstudio.github.io/DT/functions.html)

```{r echo = TRUE, message = FALSE}

DT::datatable(tf_idf, filter = 'top') %>% 
  formatRound(columns = c('tf', 'idf', 
                          'tf_idf'), 
              digits = 3) %>%
  formatStyle(0, 
              target = 'row', 
              lineHeight='25%')
```

## Visualising tf-idf within newsgroups

The code chunk used to prepare the plot.

```{r echo = TRUE, message = FALSE}

tf_idf %>%
  filter(str_detect(newsgroup, "^sci\\.")) %>% # change the "^sci\\." if you want to focus on other newsgroup
  
  group_by(newsgroup) %>%
  slice_max(tf_idf, 
            n = 12) %>%
  ungroup() %>%
  mutate(word = reorder(word, 
                        tf_idf)) %>%
  ggplot(aes(tf_idf, 
             word, 
             fill = newsgroup)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ newsgroup, 
             scales = "free") +
  labs(x = "tf-idf", 
       y = NULL)
```

## Discovering correlation pairs among words using the widyr package

```{r echo = TRUE, message = FALSE}

newsgroup_cors <- words_by_newsgroup %>%
  pairwise_cor(newsgroup, 
               word, 
               n, 
               sort = TRUE)

head(newsgroup_cors,3)

```

```{r echo = TRUE, message = FALSE}

set.seed(2017)
newsgroup_cors %>%
  filter(correlation > .025) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation, 
                     width = correlation)) +
  geom_node_point(size = 6, 
                  color = "lightblue") +
  geom_node_text(aes(label = name),
                 color = "red",
                 repel = TRUE) +
  theme_void()
```


## using Bigrams - which words are often joined together

```{r echo = TRUE, message = FALSE}

bigrams <- cleaned_text %>%
  unnest_tokens(bigram, 
                text, 
                token = "ngrams", 
                n = 2)

head(newsgroup_cors,3)
```
```{r echo = TRUE, message = FALSE}

bigrams_count <- bigrams %>%
  filter(bigram != 'NA') %>%
  count(bigram, sort = TRUE)

bigrams_count
```
Cleaning the bigrams to remove common english words like conjunctions

```{r echo = TRUE, message = FALSE}

bigrams_separated <- bigrams %>%
  filter(bigram != 'NA') %>%
  separate(bigram, c("word1", "word2"), 
           sep = " ")

glimpse (bigrams_separated)

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

head (bigrams_filtered,3)
```

```{r echo = TRUE, message = FALSE}

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
```


# Create a network graph from bigram data frame

```{r echo = TRUE, message = FALSE}

bigram_graph <- bigram_counts %>%
  filter(n > 3) %>%
  graph_from_data_frame()

bigram_graph
```
# Plotting the bigram

```{r echo = TRUE, message = FALSE}

set.seed(1234)

a <- grid::arrow(type = "closed", 
                 length = unit(.15,
                               "inches"))
ggraph(bigram_graph, 
       layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), 
                 show.legend = FALSE,
                 arrow = a, 
                 end_cap = circle(.07,
                                  'inches')) +
  geom_node_point(color = "lightblue", 
                  size = 5) +
  geom_node_text(aes(label = name), 
                 vjust = 1, 
                 hjust = 1) +
  theme_void()
```

# Importing words from a ppt file

```{r echo = TRUE, message = FALSE}

install.packages("officer")
library(officer)

transformationpptx <- system.file(package = "officer", "data/Chpt 8 Slides_for prof.pptx")


doc <- read_pptx(transformationpptx)
content <- pptx_summary(doc)
content

```






